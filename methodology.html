<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>StrideScan | Methodology</title>
    <link rel="icon" type="image/x-icon" href="/media/faviconstride.png">
    <link rel="stylesheet" href="style.css">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
  </head>
  <body>
    <nav class="navbar fixed-top navbar-expand-lg navbar-dark shadow-5-strong">
      <div class="container-fluid">
        <!--<a class="navbar-brand" href="#">Navbar</a>-->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
          <ul class="navbar-nav">
            <li class="nav-item">
              <a class="nav-link" aria-current="page" href="../">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="./download">Download</a>
            </li>
            <li class="nav-item">
              <a class="nav-link active" href="./methodology">Methodology</a>
            </li>
            <!--<li class="nav-item">
              <a class="nav-link" href="#">Tutorial</a>
            </li>-->
          </ul>
        </div>
      </div>
    </nav>
    <div class="background"></div>
      <div class="maincontain maincont">
        
        
        <h1 class="header headmethod">Methodology</h1>
        <p class="welcometext">Here I detail the process of training the computer vision model and the analytical techniques that power the app’s insightful feedback on sprinting form.</p>
        

      </div>

      <div class="inspirtation topneg">
        <h1 class="inspirationtitle">The Model</h1>
        <ul class="aboutlist">
            <li>
                <strong>Gathering Tailored Data:</strong>
                The model is trained on a set of images I personally captured at different tracks. This approach allowed me to gather a specialized dataset with a broad range of sprinting styles and conditions. By photographing athletes in various positions and angles, I ensured a varied dataset that reflects the complexities of sprinting.
            </li>
            <li>
                <strong>Fine-Tuning for Sprint Analysis:</strong>
                Leveraging the YOLOv8 model and utilizing Python, I fine-tuned the CNN to specialize in detecting and analyzing crucial aspects of sprinting form, such as limb positioning and torso alignment. This fine-tuning process was essential to shift the model’s focus from general object detection to the specific nuances of athletic movement analysis.
            </li>
            <li>
                <img src="media/modeldemo.png" class="modeldemo">
            </li>
          </ul>
          <h1 class="inspirationtitle">Assumed Invariants</h1>
          <div class="newsetuplist">
              <strong>Here are some of the things I assumed were always true to make the program simpler and more effective.</strong>
              <ol>
                <li>The camera doesn't change position or angle throughout the video.</li>
                <li>The runner is generally consistent with their form regardless of its quality.</li>
                <li>The camera angle is perpendicular to the runner's direction.</li>
                <li>The runner is in their drive stage (top speed) for the entirety of the video.</li>
                <li>The runner is the only person in the video.</li>
                <li>The runner stays on screen for at least 2 seconds.</li>
                <li>The runner goes on screen once, and then doesn't reappear after.</li>
              </ol>
         </div>
          <h1 class="inspirationtitle">Thought Process</h1>
            <ul class="aboutlist">
                <li>
                <h2 class="smalltitle">Angles</h2>
                <p>I found that determining angles between body parts from the data generated by the model is relatively simple and extremely insightful. It allows me to extract the specific types of feedback I want for all runners, regardless of size, and without dealing with quantitative measurements which I will get into later.</p>
                </li>
                <li>
                <h2 class="smalltitle">Qualitative Distances</h2>
                <p>The process of evaluating qualitative distances between body parts relies on the establishment of fixed reference points, or 'anchors.' Instead of measuring specific distances, this approach involves comparing the position of one body part to another based on predefined anchor points.Much like determining angles, this method allows the program to find boolean truths about the runner's form that aren't affected by the variance in size of different runners.</p>
                </li>
                <li>
                    <h2 class="smalltitle">Going Forward With Quantitative Distances</h2>
                    <p>While finding quantitative distances are significantly less valuable in giving feedback because of their dependence on the size and proportions of the runner, they can still provide useful data for users who want to do their own analysis. But getting this data is much more complex because users will take their videos from different distances and perspectives.</p><p>My solution, which is still being prototyped, is to calculate the distance in pixles between a predetermined pair of keypoints, say, the hip to shoulder, and then compare its distance to the target distances. Then the user could simply input the true distance (they would have to measure this) between the predetermined pair of points to find the target data using ratios.</p>
                </li>
                <li>
                    <h2 class="smalltitle">Second Pass Approach</h2>
                    <p>Another idea I've been tinkering with is a second pass approach, where before the main model goes through the video and allows the data to be analyzed, a less sophisticated, but more efficient, model will go through first and determine some preliminary information about the video and the runner. Right now, the only useful info that could be gathered from this approach is the direction that the sprinter is running in (left to right/right to left), so I've streamlined the process by having users enter the direction beforehand.</p><p>But if quantitative data were to be collect as mentioned above, finding the average distance between the predetermined pair of keypoints in the first pass might resolve a lot of complexity in the program and could potentially be worth doing.</p>
                </li>
            </ul>
            <h1 class="inspirationtitle">Results</h1>
            <div class="newsetuplist inspobottom">
                This program is able to return feedback based on:
                <ol>
                    <li>The optimal height of the knees during the sprint.</li>
                    <li>The angle of the arms, suggesting improvements for optimal form.</li>
                    <li>The lean of the torso, offering suggestions to lean forward or backward for better posture.</li>
                    <li>The symmetry and coordination between left and right arm movements.</li>
                    <li>Hand positioning relative to the eyes, indicating if arms are being raised too high or not high enough.</li>
                    <li>Stride analysis, which examines the angle between the thighs to suggest a wider or narrower stride for efficiency.</li>
                    <li>Visual cues indicating the alignment of shoulders and hips for maintaining balance.</li>
                </ol>
            </div>

            <p class="inspirationtext borde">If you encounter any issues or have any questions, please feel free to open an issue on this repository or contact me at <a href="mailto:isaac.saxonov@gmail.com">isaac.saxonov@gmail.com</a>.</p>

            <p class="inspirationtext">Thank you for using StrideScan!</p>
      </div>




      <div class="newfooter">
        <p class="footertext ">© 2023 Isaac Saxonov. All rights reserved.</p>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/umd/popper.min.js" integrity="sha384-I7E8VVD/ismYTF4hNIPjVp/Zjvgyol6VFvRkX/vR+Vc4jQkC+hVqc2pM8ODewa9r" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.min.js" integrity="sha384-BBtl+eGJRgqQAUMxJ7pMwbEyER4l1g+O15P+16Ep7Q9Q+zqX6gSbd85u4mG4QzX+" crossorigin="anonymous"></script>
    
  
  </body>
</html>